---
title: 走进计算机
category: 计算机基础
author: JQiue
---

组成到底是学什么呢？其实有点像装电脑

## 电脑的基本硬件组成

电脑必须要有三大件，CPU、内存和主板

CPU 是电脑中最核心的硬件，它的全名叫做中央处理器（Central Processing Unit），电脑中所有的计算都是交给 CPU 进行的，同时 CPU 几乎是整个电脑中造价最贵的一部分

第二个是内存（Memory），打开的程序，浏览器，游戏，都要加载到内存中去才能运行，内存越大，放的东西越多，而 CPU 就是通常处理内存中数据，但是 CPU 不能直接插到内存上

主板提供了 CPU 和内存连接的通道，主板是一个有各种各样插槽的配件，CPU 和内存都需要插到主板上，主板的芯片组和总线解决了 CPU 和内存之间如何通信的问题，芯片组控制了数据传输的流转，总线是数据传输的通道

当有了这三大件，只要配上电源供电，计算机就可以运行起来了，但是还缺少一些输入输出设备，也就是 I/O 设备，比如没有显示器就看不到计算机输出的图像、文字等信息，这就是所谓的输出设备。鼠标键盘也不能少，这样才能像计算机中输入数据，它们也就是所谓的输出设备

最后可能需要一个硬盘，这样数据才能够永久的保存下来，且大部分人都会使用机箱，装上风扇，解决灰尘和散热问题，不过它们都不是计算机的必备硬件，也就是说拿鞋盒当机箱都没有问题

如果想要打游戏，可能还需要配上显卡，显卡也是必不可少的，虽然绝大部分主板自带显卡，但是如果要玩游戏，或者做深度学习，多半就要买一张独立的显卡插在主板上，显卡也里有一个处理器，也就是 GPU，它也可以做运算

## 冯诺依曼体系结构

虽然一台电脑就这么组装起来了，但是平常使用的智能机也是这样的吗？

手机由于体积原因，并没有什么所谓的 CPU 插槽和内存插槽这些东西，所以手机制造商将这些东西都封装到一个芯片里，然后嵌入手机的主板上，这种方式叫做 SoC（System on a Chip），这样看来个人电脑和手机可能组成方式不太一样，但是程序的运行方式没什么区别，都是将程序加载到内存中执行。这是因为个人电脑，手机等设备都遵循着一个概念，即冯诺依曼体系结构，也叫存储程序计算机

它有两种概念，一个是“可编程”，一个是“存储”

计算机是由各种门电路组成的，通过组装成一个电路板，来完成特定的程序，一旦需要修改程序，就必须重新组装电路，这样的计算就就是不可编程的，因为程序在硬件层面上被写死了，比如计算器，他只能做加减乘除，做不了任何计算逻辑固定之外的事，如果将程序本身存储在内存中，然后通过加载不同的程序来解决不同的问题，那么就实现了可编程

冯诺依曼体系结构即**将程序指令和数据一起存储的计算机设计概念结构**，早期计算机只能运行固定用途的程序，改变程序必须更改结构，重新设计电路，在当时重启程序并不像现在重新编译一样简单，这就导致计算机非常局限，于是冯诺依曼提出了将程序存储起来，并设计通用的电路，运行程序的时候，把程序当成电路能够理解的语言，然后让通用电路执行相关的逻辑，这就是冯诺依曼体系的核心概念：**存储程序指令，并设计通用电路**

::: tip 现代计算机结构
现代计算机结构从本质上来讲还是冯诺依曼结构，但是对原有的结构进行了改变来解决冯诺依曼原有结构 CPU 和存储设备之间的性能差异，在这个结构中，存储器，运算器，控制器在一个结构上面，现在计算机的结构可以理解为**以存储器为核心的结构**
:::

## 第一台通用计算机的诞生

ENIAC 是 1946 年在美国宾夕法尼亚大学制成的，但实际上真正意义上的第一台计算机是阿塔纳索夫-贝瑞计算机，ENIAC 发明者是从阿塔纳索夫继承了电子数字计算机的主要设计构想，因此阿塔纳索夫-贝瑞计算机是世界上第一台计算机，而 ENIAC 是通用的第一台电子数字计算机，因为 ENIAC 是可编程的计算机

::: center
<img src="https://gitee.com/jqiue/img_upload/raw/master/images/firstcomputer.jpg"/>
:::

## 计算机的分类

+ 电子模拟计算机：模拟计算机是通过电压来表示数据，并用盘上连线控制，它的精度较低，存储量较小，没有逻辑判断能力，而数字计算机通过数字 0 和 1 表示，并通过程序来控制，它的精度非常高，存储量非常大，逻辑判断能力也非常强

+ 电子数字计算机：电子数字计算机是现在最主流的计算机，通常也被称为电脑或者电子计算机，且可以进一步分为专用计算机和通用计算机，**它们是根据计算机的效率、速度、价格、运行的经济性和适应性来划分的**

专业计算机是最有效，最经济和最快速的计算机，但是他的适应性很差，通用计算机适应性很强，但是牺牲了效率，速度和经济型，通用计算机又可以分为超级计算机，大型机，服务器，PC 机，单片机和多核机，他们的区别在于体积，简易性，功耗，性能，数据存储容量，指令系统规模和机器价格

::: center
<img src="https://gitee.com/jqiue/img_upload/raw/master/images/计算机1.1.png"/>
:::

+ 个人计算机：通常对个人用户来讲提供良好的性能，且价格低廉

+ 服务器：为多用户运行大型程序的计算机，执行大负载任务，通常是多个用户并行访问，提供比个人计算机更强的性能以及更加可靠的稳定性。低端服务器一般用于商务应用或者 Web 服务，而高端服务器又叫做**超级计算机**，常用于科学计算，这类计算机代表着最高价格以及最高的计算性能，但在整个计算机市场中占用的比例较少，跟个人计算机比起来，当前民用最强 CPU 连超算的十分之一都赶不上，可谓差距之大

+ 嵌入式计算机：是数量最多的一类，常用于汽车，电视、控制飞机等设备的处理器，它的设计目标是运行单一的应用程序，对成本和功耗有严格限制，必须快速的执行有限功能，且必须保证正常运转

## 程序概念

众所周知，计算机的硬件只能执行简单的低级指令，一个复杂的应用程序到简单的指令需要经过多个软件层次来将高层次的操作翻译成简单的低级指令，这是一种抽象的解释。在层次中，最外层是应用软件，中间是硬件，而系统软件在两者之间

### 系统软件

系统软件提供操作系统、编译程序、加载程序、汇编程序等，对于现代计算机系统来说，**操作系统**和**编译程序**是必须的。操作系统是用户程序和硬件之间的接口，提供了基本的输入和输出，分配内外存，为应用程序提供计算机资源的操作

编译程序用于将高级语言编写的程序翻译成硬件能够执行的指令，对于计算机来说最简单的信号是“通”和”断“，所以计算机用两个符号 0 和 1 表示。也因此计算机的语言通常被称之为**二进制数**，每个符号就是一个**二进制位**或**一位**（bit），而计算机指令就是一个个二进制位组成的位串

### 转向高级语言

最初程序员是通过二进制来和计算机打交道的，由于二进制太过难以理解，它们很快发明了助记符，最初助记符是手工翻译成二进制，后来开发了一种**汇编程序**，将助记符自动翻译成对应的二进制，这种符号语言又叫**汇编语言**，而计算机理解的二进制语言叫**机器语言**

后来人们意识到可以编写一个能够将接近人类语言翻译成计算机指令的程序，于是就设计了高级语言的编译程序，使用高级编程语言大大提高了软件的开发效率，这些高级语言由一些单词和代数符号组成，可以由编译器转换成汇编语言，再由汇编器转换成机器语言交给计算机执行

## 计算机的性能

对于计算机来说，如何评价它的性能是很重要的，但是现代计算机采用了大量先进的性能改进方法，使性能评价极为困难

### 性能的定义

如果在两台计算机中运行同一个程序，可以认为首先完成的那台计算机更快，如果是服务器，则应该是完成量最多的那台计算机更快。个人计算机强调响应时间，服务器则强调吞吐率。因此对于不同的计算机来说，应该采用不同的性能度量标准

::: tip 响应时间 && 吞吐率

+ 响应时间：指计算机完成某个任务的总时间
+ 吞吐率：指单位时间内完成的任务数量
:::

因此想要增加计算机的性能，就要减少响应时间，增加吞吐率，但是通常减少响应时间也提升了吞吐率

一般将性能定义为响应时间的倒数：

::: center
性能 = 1 / 响应时间
:::

响应时间越短，性能数值越大，如果一个程序在处理器 A 上需要 6s 完成，而在处理器 B 上需要 3s 完成，根据公式算出，A 的 性能是 1/6，而 B 的性能是 1/3，A 和 B 的性能比为 2，那么就说明 B 的性能是 A 的两倍

时间虽然能够衡量性能，但有两个问题：

+ 时间不准：因为一个程序不一定每次运行时间是一样的，因为 CPU 实际上在各个程序之间切换
+ CPU 性能：CPU 可能满载运行，也可能降频运行，导致时间有所差异

所以因该对 CPU 时间进行拆解，将执行时间变成时钟周期数和时钟周期时间的乘积：

::: center
程序的 CPU 执行时间 = CPU 时钟周期数 * CPU 时钟周期时间
:::

时钟周期时间就是 CPU 主频，比如 AMD 3200G 的主频是 3.6GHz，可以简单粗暴的理解为，CPU 在 1 秒内执行的指令是 3.6G条，CPU 内部是由晶振驱动的，类似于墙上的挂钟，“滴答滴答”一秒一秒的走，通过挂钟就能识别出来最小的单位是秒。而 CPU 每一次的震荡时间就是时钟周期时间，只要这个主频越高就代表它走的越快，因此最简单的性能提升方式就是缩短时钟周期时间，但是对于人磊说这个操作相当于更换了 CPU，因此将目光瞄向了时钟周期数，如果能减少程序需要的时钟周期数，一样能够提升性能

对于周期数来，受到程序的指令数影响较大，所以可以进一步拆解成“指令数 * 每条指令的平均时钟周期数”，简称 CPI，这样上面的公式就变成了：

::: center
程序的 CPU 执行时间 = 指令数 * CPI * CPU 时钟周期时间
:::

除了优化这些东西来提升性能，还可以将一个芯片集成多个 CPU 来提升性能，所以就出现了向单核处理器向多核处理器的转变

## 功耗墙

对于 CPU 来说，只要在 CPU 多放一些晶体管，提高时钟频率就能够让它更快，但是对于 CPU 来说是有功耗上限的，因为同一个体积的性能提升是有限的，仅仅通过堆硬件的方式，在如今不能够满足对程序性能的需求了，所以需要从以下几个方面入手：

+ 加速大概率事件
+ 通过流水线提高性能
+ 通过预测提高性能

## 计算机的计算单位

### 容量单位

bit|Byte|KB|MB|GB|TB|PB|EB
---|---|---|---|---|---|---|---
比特位|字节|千字节|兆字节|吉字节|太字节|拍字节|艾字节
-|8bits|1024B|1024KB|1024MB|1024GB|1024TB|1024EB
门电路|-|寄存器|高速缓存|内存/硬盘|硬盘|云硬盘|数据仓库

::: warning
在千字节之后，所有的单位的换算关系都是 1024，只有字节和比特位使用的是 8 进制位，同时 1024 = 2^10^
:::

::: danger 为什么网上买的硬盘容量标称 500G，格式化之后变成了 465G？
这是因为硬盘制造商使用的是 10 进制位来标记容量，也就是说 1G 等于1000MB，而不是 1024MB，所以(500 * 1000^3^)/1024^3^ 约等于 465G，硬盘商使用 1000 进制位，则是因为硬盘的扇区，也就是存储数据的地方，在记住这个扇区的容量时，使用的是人类容易理解的十进制，更容易去沟通和协商
:::

### 速度单位

+ 网络速度

在互联网走进千家万户的同时，会经常听到各种 2M、4M、以及 10M 这样的关键字，这里的单位并不是存储单位，而是一种速度单位。

::: danger 为什么电信拉的 100M 光纤，测试峰值速度只有 12M 每秒？
在网络中常用速度单位为：Mbps，100M 是 100Mbps 的缩写，而 100Mbps 等于 100Mbit/s，所以换算成实际的速度单位为：100Mbit/s = (100/8)MB/s = 12.5MB/s
:::

## 字符与编码集

### ASCII 码

7 个 bits 就可以完全表示 ASCII 码，其中包括 95 个可打印字符，33 个不可打印字符（包括控制字符）：

::: center
33 + 95 = 128 = 2^7^
:::

::: warning 缺陷
ASCII 的局限在于只能显示 26 个基本拉丁字母、阿拉伯数字和英式标点符号，因此只能用于显示现代美国英语，对其他语言依然无能为力
:::

### ASCII 码的扩展性

随着时间的发展，ASCII 码逐渐的满足不了更多的需求了，主要表现在很多应用或者国家中的符号都无法表示，这时就对 ASCII 码进行了第一次扩充，将 7bits 增加到 8bits，就可以让 ASCII 码表示 256 个字符，于是产生了可扩展的 ASCII 码

### 字符编码集的国际化

在世界上，很多语言的体系都不一样，很多都是不以有限字符组合的语言，比如英文只用 26 个英文字母表示，而中文除了偏旁以外，每一个字都是独立的，这些字符尤以中国、韩国、日本等国家的语言最为复杂，所以字符编码集的国际化非常有必要

#### 中文编码集

+ GB2312：又叫《信息交换用汉字编码字符集——基本集》，一共收录了 7445 个字符，包括 6763 个汉字和 682 个其他符号，虽然 GB2312 算是一个比较完备的中文字符集，但是并不符合国际标准
+ GBK：又叫《汉字内码扩展规范》，向下兼容 GB2312，向上支持国际 ISO 标准，收录了 21003 个汉字，支持中日韩全部字符

不管中文编码如何完善，只是一个本地化的编码，跨国使用都会出现乱码现象

### Unicode

又被称为统一码，万国码，单一码，从字面意思来看，Unicode 可以表示全世界所有的字符，Unicode 定义了世界通用的符号集，UTF-* 实现了编码，UTF-8 以字节为单位对 Unicode 进行编码

::: tip
在中国的 Windows 系统是默认使用 GBK 编码，编程时必须注意这个问题
:::
